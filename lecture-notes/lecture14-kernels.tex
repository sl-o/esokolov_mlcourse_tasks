\documentclass[12pt,fleqn]{article}
\usepackage{vkCourseML}
\hypersetup{unicode=true}
%\usepackage[a4paper]{geometry}
\usepackage[hyphenbreaks]{breakurl}

\interfootnotelinepenalty=10000

\begin{document}
\title{Лекция 14\\Ядра в машинном обучении}
\author{Е.\,А.\,Соколов\\ФКН ВШЭ}
\maketitle

\section{Ядровой SVM}

Вспомним, что метод опорных векторов сводится к решению задачи оптимизации
\begin{equation}
\label{eq:svmUnsep}
    \left\{
        \begin{aligned}
            & \frac{1}{2} \|w\|^2 + C \sum_{i = 1}^{\ell} \xi_i \to \min_{w, b, \xi} \\
            & y_i \left(
                \langle w, x_i \rangle + b
            \right) \geq 1 - \xi_i, \quad i = 1, \dots, \ell, \\
            & \xi_i \geq 0, \quad i = 1, \dots, \ell.
        \end{aligned}
    \right.
\end{equation}

Построим двойственную к ней.
Запишем лагранжиан:
\[
    L(w, b, \xi, \lambda, \mu)
    =
    \frac{1}{2} \|w\|^2 + C \sum_{i = 1}^{\ell} \xi_i
    -
    \sum_{i = 1}^{\ell} \lambda_i \left[
        y_i \left(
                \langle w, x_i \rangle + b
            \right) - 1 + \xi_i
    \right]
    -
    \sum_{i = 1}^{\ell}
        \mu_i \xi_i.
\]
Выпишем условия Куна-Таккера:
\begin{alignat}{3}
    \label{eq:svmKktW}
    & \nabla_w L = w - \sum_{i = 1}^{\ell} \lambda_i y_i x_i = 0
    & \quad\Longrightarrow\quad
    & w = \sum_{i = 1}^{\ell} \lambda_i y_i x_i \\
    %
    \label{eq:svmKktB}
    & \nabla_b L = - \sum_{i = 1}^{\ell} \lambda_i y_i = 0
    & \quad\Longrightarrow\quad
    & \sum_{i = 1}^{\ell} \lambda_i y_i = 0 \\
    %
    \label{eq:svmKktXi}
    & \nabla_{\xi_i} L = C - \lambda_i - \mu_i
    & \quad\Longrightarrow\quad
    & \lambda_i + \mu_i = C \\
    %
    \label{eq:svmKktSlack1}
    & \lambda_i \left[
        y_i \left(
                \langle w, x_i \rangle + b
            \right) - 1 + \xi_i
        \right] = 0
    & \quad\Longrightarrow\quad
    & (\lambda_i = 0)
        \ \text{или}\
        \left(
            y_i \left(
                \langle w, x_i \rangle + b
            \right)
            =
            1 - \xi_i
        \right) \\
    %
    \label{eq:svmKktSlack2}
    & \mu_i \xi_i = 0
    & \quad\Longrightarrow\quad
    & (\mu_i = 0)
        \ \text{или}\
        (\xi_i = 0) \\
    %
    &\xi_i \geq 0, \lambda_i \geq 0, \mu_i \geq 0.
\end{alignat}

Проанализируем полученные условия.
Из~\eqref{eq:svmKktW} следует, что вектор весов, полученный в результате
настройки SVM, можно записать как линейную комбинацию объектов,
причем веса в этой линейной комбинации можно найти как решение двойственной задачи.
В зависимости от значений~$\xi_i$ и~$\lambda_i$ объекты~$x_i$ разбиваются на три категории:
\begin{enumerate}
    \item $\xi_i = 0$, $\lambda_i = 0$. \\
        Такие объекты не влияют решение~$w$~(входят в него с нулевым весом~$\lambda_i$),
        правильно классифицируются~($\xi_i = 0$) и лежат вне разделяющей полосы.
        Объекты этой категории называются~\emph{периферийными}.
    \item $\xi_i = 0$, $0 < \lambda_i < C$. \\
        Из условия~\eqref{eq:svmKktSlack1} следует, что~$y_i \left(\langle w, x_i \rangle + b \right) = 1$,
        то есть объект лежит строго на границе разделяющей полосы.
        Поскольку~$\lambda_i > 0$, объект влияет на решение~$w$.
        Объекты этой категории называются~\emph{опорными граничными}.
    \item $\xi_i > 0$, $\lambda_i = C$. \\
        Такие объекты могут лежать внутри разделяющей полосы~($0 < \xi_i < 2$)
        или выходить за ее пределы~($\xi_i \geq 2$).
        При этом если~$0 < \xi_i < 1$, то объект классифицируется правильно,
        в противном случае~--- неправильно.
        Объекты этой категории называются~\emph{опорными нарушителями}.
\end{enumerate}

Отметим, что варианта~$\xi_i > 0$, $\lambda_i < C$ быть не может,
поскольку при~$\xi_i > 0$ из условия дополняющей нежесткости~\eqref{eq:svmKktSlack2}
следует, что~$\mu_i = 0$, и отсюда из уравнения~\eqref{eq:svmKktXi} получаем,
что~$\lambda_i = C$.

Итак, итоговый классификатор зависит только от объектов, лежащих
на границе разделяющей полосы, и от объектов-нарушителей~(с~$\xi_i > 0$).

Построим двойственную функцию.
Для этого подставим выражение~\eqref{eq:svmKktW} в лагранжиан,
и воспользуемся уравнениями~\eqref{eq:svmKktB} и~\eqref{eq:svmKktXi}~(данные
три уравнения выполнены для точки минимума лагранжиана при
любых фиксированных~$\lambda$ и~$\mu$):
\begin{align*}
    L &= \frac{1}{2} \left\|
            \sum_{i = 1}^{\ell}
                \lambda_i y_i x_i
        \right\|^2
        -
        \sum_{i, j = 1}^{\ell}
            \lambda_i \lambda_j y_i y_j \langle x_i, x_j \rangle
        -
        b
        \underbrace{\sum_{i = 1}^{\ell}
            \lambda_i y_i}_{0}
        +
        \sum_{i = 1}^{\ell}
            \lambda_i
        +
        \sum_{i = 1}^{\ell}
            \xi_i \underbrace{(C - \lambda_i - \mu_i)}_{0} \\
    &=
    \sum_{i = 1}^{\ell}
        \lambda_i
    -
    \frac{1}{2} \sum_{i, j = 1}^{\ell}
        \lambda_i \lambda_j y_i y_j \langle x_i, x_j \rangle.
\end{align*}

Мы должны потребовать выполнения условий~\eqref{eq:svmKktB}
и~\eqref{eq:svmKktXi}~(если они не выполнены,
то двойственная функция обращается в минус бесконечность),
а также неотрицательность двойственных
переменных~$\lambda_i \geq 0$, $\mu_i \geq 0$.
Ограничение на~$\mu_i$ и условие~\eqref{eq:svmKktXi},
можно объединить, получив~$\lambda_i \leq C$.
Приходим к следующей двойственной задаче:
\begin{equation}
\label{eq:svmDual}
    \left\{
        \begin{aligned}
            & \sum_{i = 1}^{\ell}
                \lambda_i
            -
            \frac{1}{2} \sum_{i, j = 1}^{\ell}
                \lambda_i \lambda_j y_i y_j \langle x_i, x_j \rangle
            \to \max_{\lambda} \\
            & 0 \leq \lambda_i \leq C, \quad i = 1, \dots, \ell, \\
            & \sum_{i = 1}^{\ell} \lambda_i y_i = 0.
        \end{aligned}
    \right.
\end{equation}
Она также является вогнутой, квадратичной и имеет единственный максимум.

Двойственная задача SVM зависит только от скалярных произведений объектов~---
отдельные признаковые описания никак не входят в неё.
Значит, можно легко сделать ядровой переход:
\begin{equation}
    \left\{
        \begin{aligned}
            & \sum_{i = 1}^{\ell}
                \lambda_i
            -
            \frac{1}{2} \sum_{i, j = 1}^{\ell}
                \lambda_i \lambda_j y_i y_j K(x_i, x_j)
            \to \max_{\lambda} \\
            & 0 \leq \lambda_i \leq C, \quad i = 1, \dots, \ell, \\
            & \sum_{i = 1}^{\ell} \lambda_i y_i = 0.
        \end{aligned}
    \right.
\end{equation}

Вернемся к тому, какое представление классификатора дает двойственная задача.
Из уравнения~\eqref{eq:svmKktW} следует, что вектор весов~$w$
можно представить как линейную комбинацию объектов из обучающей выборки.
Подставляя это представление~$w$ в классификатор, получаем
\begin{equation}
\label{eq:svmDualClassifier}
    a(x) = \sign \left(
        \sum_{i = 1}^{\ell} \lambda_i y_i \langle x_i, x \rangle + b
    \right).
\end{equation}
Таким образом, классификатор измеряет сходство нового объекта
с объектами из обучения, вычисляя скалярное произведение между ними.
Это выражение также зависит только от скалярных произведений,
поэтому в нём тоже можно перейти к ядру.

В представлении~\eqref{eq:svmDualClassifier} фигурирует переменная~$b$,
которая не находится непосредственно в двойственной задаче.
Однако ее легко восстановить по любому граничному опорному объекту~$x_i$,
для которого выполнено~$\xi_i = 0, 0 < \lambda_i < C$.
Для него выполнено~$y_i \left(\langle w, x_i \rangle + b \right) = 1$,
откуда получаем
\[
    b = y_i - \langle w, x_i \rangle.
\]
Как правило, для численной устойчивости берут медиану данной величины по
всем граничным опорным объектам:
\[
    b = \med \{ y_i - \langle w, x_i \rangle \cond \xi_i = 0, 0 < \lambda_i < C \}.
\]

\paragraph{Связь с kNN.}
Если использовать гауссовское ядро~(или, как его еще называют, RBF-ядро)
в методе опорных векторов, то получится следующее решающее правило:
\[
    a(x)
    =
    \sign
    \sum_{i = 1}^{\ell}
        y_i \lambda_i
        \exp\left(
            -\frac{
                \|x - x_i\|^2
            }{
                2 \sigma^2
            }
        \right).
\]

Вспомним теперь, что решающее правило в методе~$k$ ближайших соседей выглядит как
\[
    a(x)
    =
    \argmax_{y \in Y}
        \Gamma_y(x, X^\ell);
    \quad
    \Gamma_y(x, X^\ell)
    =
    \sum_{i = 1}^{\ell}
        [y^{(i)}_x = y]
        w(i, x),
\]
где~$w(i, x)$~--- оценка важности~$i$-го соседа для классификации объекта~$x$,
а~$y^{(i)}_x$~--- метка~$i$-го ближайшего соседа.
Для случая двух классов~$\{+1, -1\}$ решающее правило можно записать
как знак разности оценок за эти классы:
\begin{align*}
    a(x)
    &=
    \sign\left(
        \Gamma_{+1}(x, X^\ell)
        -
        \Gamma_{-1}(x, X^\ell)
    \right)
    =\\
    &=
    \sign\left(
        \sum_{i = 1}^{\ell}
            [y^{(i)}_x = +1]
            w(i, x)
        -
        \sum_{i = 1}^{\ell}
            [y^{(i)}_x = -1]
            w(i, x)
    \right)
    =\\
    &=
    \sign \sum_{i = 1}^{\ell}
        ([y^{(i)}_x = +1] - [y^{(i)}_x = -1])
        w(i, x)
    =\\
    &=
    \sign \sum_{i = 1}^{\ell}
        y^{(i)}_x
        w(i, x).
\end{align*}

Заметим, что решающие правила метода опорных векторов с RBF-ядром
и метода~$k$ ближайших соседей совпадут, если положить
\[
    w(i, x)
    =
    \lambda_{(i)}
    \exp\left(
        -\frac{
            \|x - x_{(i)}\|^2
        }{
           2 \sigma^2
        }
    \right).
\]
То есть SVM-RBF~--- это метод~$\ell$ ближайших соседей,
использующий гауссово ядро в качестве функции расстояния,
и настраивающий веса объектов путем максимизации отступов.

\section{Аппроксимация спрямляющего пространства}

Все ядровые методы используют матрицу Грама~$G = X X^T$ вместо
матрицы~<<объекты-признаки>>~$X$.
Это позволяет сохранять сложность методов при сколь
угодно большой размерности спрямляющего пространства,
но работа с матрицей Грама для больших выборок может стать затруднительной.
Так, уже при выборках размером в сотни тысяч объектов хранение этой матрицы
потребует большого количества памяти, а обращение станет трудоёмкой задачей,
поскольку требует~$O(\ell^3)$ операций.

Решением данной проблемы может быть построение в явном виде такого преобразования~$\tilde \phi(x)$,
которое переводит объекты в пространство не очень большой размерности,
и в котором можно напрямую обучать любые модели.
Мы разберём метод случайных признаков Фурье~(иногда также называется Random Kitchen Sinks)~\cite{rahimi07rks},
который обладает свойством аппроксимации скалярного произведения:
\[
    \langle \tilde \phi(x), \tilde \phi(z) \rangle
    \approx
    K(x, z).
\]

%\subsection{Метод Нистрома}
%Данный метод~\cite{drineas05nystroem} сводится к выбору случайного подмножества объектов обучающей выборки~$x_1, \dots, x_n$.
%На их основе строится преобразование
%\[
%    \tilde \phi(x)
%    =
%    \left(
%        K(x, x_1),
%        \dots,
%        K(x, x_n)
%    \right).
%\]
%Также с помощью сэмплированных объектов можно построить~$k$-ранговую аппроксимацию
%полной матрицы Грама~$G$~--- это делается по формуле
%\[
%    G
%    \approx
%    \tilde G
%    =
%    C W_k^+ C^T,
%\]
%где матрицы~$C$ и~$W$ задаются как
%\begin{align*}
%    &C = (K(x_i, x_j))_{i = 1, j = 1}^{\ell, n} \in \RR^{\ell \times n},\\
%    &W = (K(x_i, x_j))_{i = 1, j = 1}^{n, n} \in \RR^{n \times n},\\
%\end{align*}
%$W_k$~--- лучшее~$k$-ранговое приближение матрицы~$W$~(по норме Фробениуса),
%а через~$W_k^+$ обозначена псевдообратная матрица для~$W_k$~(псевдообратную можно вычислить,
%посчитав сингулярное разложение матрицы~$W_k = U \Sigma V^T$ и обратив ненулевые элементы на диагонали
%матрицы~$\Sigma$).

%Преимущество данной аппроксимации состоит в том, что обращение матрицы вида~$G + \lambda I$~(которая
%часто возникает в ядровых методах) можно свести к обращению матрицы размера~$n \times n$
%вместо~$\ell \times \ell$:
%\begin{align*}
%    (G + \lambda I)^{-1}
%    &\approx
%    (\tilde G + \lambda I)\\
%    &=
%    \frac{1}{\lambda}
%    (I - C (W_k^+ C^T C + \lambda I)^{-1} W_k^+ C^T).
%\end{align*}

%\subsection{Метод случайных признаков Фурье}

%Данный метод~\cite{rahimi07rks} предлагает альтернативный способ построения аппроксимации спрямляющего пространства.
%Вычисление признаков в нём гораздо проще, но при этом поиск их параметров несколько сложнее,
%чем в предыдущем методе.

Из комплексного анализа известно, что любое непрерывное ядро вида~$K(x, z) = K(x - z)$
является преобразованием Фурье некоторого вероятностного распределения~(теорема Бохнера):
\[
    K(x - z)
    =
    \int_{\RR^d}
        p(w)
        e^{i w^T (x - z)}
    dw.
\]
Преобразуем интеграл:
\begin{align*}
    \int_{\RR^d}
        p(w)
        e^{i w^T (x - z)}
    dw
    &=
    \int_{\RR^d}
        p(w)
        \cos(w^T (x - z))
    dw
    +
    i
    \int_{\RR^d}
        p(w)
        \sin(w^T (x - z))
    dw
    =\\
    &=
    \int_{\RR^d}
        p(w)
        \cos(w^T (x - z))
    dw.
\end{align*}
Поскольку значение ядра~$K(x - z)$ всегда вещественное,
то и в правой части мнимая часть равна нулю~---
а значит, остаётся лишь интеграл от косинуса~$\cos w^T (x - z)$.
Мы можем приблизить данный интеграл методом Монте-Карло:
\[
    \int_{\RR^d}
        p(w)
        \cos(w^T (x - z))
    dw
    \approx
    \frac{1}{n}
    \sum_{j = 1}^{n}
        \cos(w_j^T (x - z)),
\]
где векторы~$w_1, \dots, w_n$ генерируются из распределения~$p(w)$.
Используя эти векторы, мы можем сформировать аппроксимацию преобразования~$\phi(x)$:
\[
    \tilde \phi(x)
    =
    \frac{1}{\sqrt{n}}
    (\cos(w_1^T x), \dots, \cos(w_n^T x),
    \sin(w_1^T x), \dots, \sin(w_n^T x)).
\]
Действительно, в этом случае скалярное произведение новых признаков будет иметь вид
\begin{align*}
    \tilde K(x, z)
    =
    \langle \tilde \phi(x), \tilde \phi(z) \rangle
    &=
    \frac{1}{n}
    \sum_{j = 1}^{n} \left(
        \cos(w_j^T x) \cos(w_j^T z)
        +
        \sin(w_j^T x) \sin(w_j^T z)
    \right)\\
    &=
    \frac{1}{n}
    \sum_{j = 1}^{n}
        \cos(w_j^T (x - z)).
\end{align*}
Данная оценка является несмещённой для~$K(x, z)$ в силу свойств метода Монте-Карло.
Более того, с помощью неравенств концентрации меры можно показать, что дисперсия данной оценки достаточно низкая.
Например, для гауссова ядра будет иметь место неравенство для некоторых констант~$C$ и~$\varepsilon$:
\[
    \PP
    \left[
        \sup_{x, z}
        | \tilde K(x, z) - K(x, z) |
        \geq
        \eps
    \right]
    \leq
    (C / \eps)^2
    \exp(-n \eps^2 / 4(d + 2)).
\]

Разумеется, найти распределение~$p(w)$ можно не для всех ядер~$K(x - z)$.
Как правило, данный метод используется для гауссовых ядер~$\exp(\|x - z\|^2 / 2 \sigma^2)$~---
для них распределение~$p(w)$ будет нормальным  с нулевым матожиданием и дисперсией~$\sigma^2$.


\begin{thebibliography}{1}
\bibitem{rahimi07rks}
    \emph{Rahimi, Ali and Recht, Benjamin}
    Random Features for Large-scale Kernel Machines.~//
    Proceedings of the 20th International Conference on Neural Information Processing Systems,
    2007.
\end{thebibliography}

\end{document}
