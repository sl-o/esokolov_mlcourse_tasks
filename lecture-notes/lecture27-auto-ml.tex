\documentclass[12pt, fleqn]{article}

\usepackage{vkCourseML}

\usepackage{mathtools}
\usepackage{csquotes}
\hypersetup{unicode=true}
\interfootnotelinepenalty=10000
\title{Лекция 27 \\ AutoML}
\author{Е.\,А.\,Соколов \\ ФКН ВШЭ}
\date{}

\begin{document}

\maketitle

\section{Введение}

Допустим вы пришли работать и вам поставили задачу сделать классификатор обращений пользователей в службу поддержки.
Выделим основные этапы решения данной задачи:

\begin{enumerate}
	\item Сбор данных;
	\item Предварительная обработка данных;
	\item Выбор целевой метрики;
	\item Изготовление признаков;
	\item Выбор семейства моделей, подбор гиперпараметров.
\end{enumerate}

Сразу заметим, что выбор целевой метрики мы никак не сможем автоматизировать, потому что он требует понимания сути бизнес-требований.
Сбор данных тоже не факт что может быть полностью автоматизирован.
Этап предобработки данных состоит из какого-то стандартного набора действий (заполнить пропуски, выкинуть выбросы), которые можно выполнять автоматически.
Для изготовления новых признаков можно перебирать комбинации исходных признаков с применением к ним каких-то преобразований (посчитать производную, взять логарифм).
Выбор семейства моделей и подбор гиперпараметров тоже поддается автоматизации.

\section{Подбор гиперпараметров}

Пусть есть набор гиперпараметров $\theta \in \mathbb{R}^{n}$, определяющий модель, подготовку данных, и так далее.
Пусть также есть функция $f(\theta)$, которая говорит, какая ошибка получится, если взять модель и обучить её с гиперпараметрами $\theta$.
Тогда мы ищем решение задачи
\[
	f(\theta) \to \min\limits_{\theta}.
\]
Рассмотрим различные способы её решения.

\begin{description}
	\item[Grid Search:] перебрать по сетке тысячу гиперпараметров не представляется возможным в принципе --- очень долго.
	\item[Random Search:] может быть быстрее Grid Search и легче управлять вычислительными ресурсами.
	\item[Градиентный спуск:] не подходит, потому что 
	\begin{itemize}
		\item $f(\theta)$ очень долго считать: нужно пройти весь пайплайн, обучить модель, и ещё качество на отложенной выборке посчитать;
		\item $f(\theta)$ не дифференцируема, а некоторые из гиперпараметров так вообще могут быть категориальными. 
	\end{itemize}
	\item[Дискретная оптимизация:] алгоритм имитации отжига (нормально работает при $n \approx 100$), генетические алгоритмы и любой другой алгоритм дискретной оптимизации.
	Проблема одна -- они не работают, поскольку у нас много параметров, по которым мы оптимизируем.
\end{description}


\subsection{Оптимизация через суррогатные функции}

Предположим, что мы знаем $f(\theta_{1}), \ldots, f(\theta_{k})$.
Построим функцию $g \colon \Theta \to \mathbb{R}$  такую, что
\begin{itemize}
	\item Для любого $i \in \{1, \ldots, k\}$ $g(\theta_{i}) \approx f(\theta_{i})$.
	Будем надеяться, что тогда $g(\theta) \approx f(\theta)$ для любого $\theta \in \Theta$;
	\item Функция $g$ является хорошей, то есть непрерывной, сколько нужно раз дифференцируемой, и так далее.
\end{itemize}
Функцию $g$ будем называть {\it суррогатной функцией}.
Тогда
\[
	\theta_{k + 1} \coloneqq \argmin\limits_{\theta \in \Theta} g(\theta).
\]
После этого мы добавляем $f(\theta_{k + 1})$ в исходную выборку, строим новую суррогатную функцию, и так далее.

\subsection{Гауссовские процессы}

\begin{vkDef}[GPGO]
	Будем говорить, что на $\mathbb{R}^{n}$ задан {\it гауссовский процесс}, если каждому $x \in \mathbb{R}^{n}$ сопоставлена случайная величина $\xi(x) \sim \mathcal{N}(\mu(x), \sigma^{2})$ такая, что
	\[
		\mathrm{cov} \left( \xi(x_{1}), \xi(x_{2}) \right) = K(x_{1}, x_{2}).
	\]
\end{vkDef}
На самом деле это не вполне корректное определение, и мы хотим потребовать
\[
	\left( \xi(x_{1}), \ldots, \xi(x_{n}) \right) \sim \mathcal{N} \left( \begin{bmatrix}
		\mu(x_{1}) \\
		\vdots \\
		\mu(x_{n})
	\end{bmatrix}, \Sigma \right),
\]
где $\Sigma$ вычисляется через $K$.

\paragraph{Интуиция}

Пусть у нас есть пространство $\mathbb{R}^{2}$, в нем в каждой точке задана случайная величина.
Допустим, мы знаем, что $\xi(x_{1}) = 10$.
Возьмем какую-то точку $x_{2}$ рядом с $x_{1}$, для которой мы еще не знаем $\xi(x_{2})$.
Но, поскольку мы знаем, что это все гауссовский процесс, то мы гарантируем, что точка $x_{2}$ будет сильно скореллирована с $x_{1}$, поэтому мы можем с помощью ядра $K$ оценить значение $\xi(x_{2})$.
Чем дальше от $x_{1}$ мы возьмем $x_{2}$, тем больше будет дисперсия наших представлений об этой точке, и тем меньше мы будем знать информации о ней.

\paragraph{Пример ядра $K$}

В выкладках выше можно взять $K$ равным ядру Матерна, то есть
\[
	K_{\rm Matern}(x_{1}, x_{2}) = \sigma^{2}_{f} \frac{2^{1 - \nu}}{\Gamma(\nu)} \left(\sqrt{2 \nu} r(x_{1}, x_{2})\right)^{\nu} K_{\nu}\left(\sqrt{2\nu} r(x_{1}, x_{2})\right),
\]
где $\sigma^{2}_{f}$ -- гиперпараметр, $r(x_{1}, x_{2}) = \sqrt{(x_{1} - x_{2})^{\top} \Lambda (x_{1} - x_{2})}$ -- расстояние Махаланобиса с гиперпараметром $\Lambda$, $K_{\nu}(\cdot)$ -- функция Бесселя 2-го рода, $\nu$ -- гиперпараметр.

Это все нам нужно, чтобы мы могли записать распределение в какой-то точке при условии значений в других точках нашего пространства.
Допустим у нас есть выборка гиперпараметров $\theta_{1}, \ldots, \theta_{k}$, которые мы уже попробовали.
Также у нас есть набор значений $f(\theta_{1}), \ldots, f(\theta_{k})$ функции потерь для этого набора гиперпараметров.
Тогда мы можем посчитать $p(f(\theta) \mid \theta_{1}, \ldots \theta_{k}, f(\theta_{1}), \ldots, f(\theta_{k}))$ и таким образом оценить неопределенность на наших значениях гиперпараметров.

Здесь нужно ответить на важный вопрос: какую следующую точку брать?
Нам нужно выбрать функцию, которая будет отвечать на вопрос, какую следующую точку брать.
Например,
\[
	\lambda(f(\theta)) = \min (f(\theta), \eta),
\]
где $\eta$ -- лучшее значение $f$, найденное на данный момент.
То есть мы хотим как можно сильнее уменьшить значение ошибки относительно наилучшего значения ошибки, найденного на этот момент.
Далее будем решать следующую задачу оптимизации:
\[
	\mathbb{E} \lambda (f(\theta)) \to \min\limits_{\theta}.
\]

\paragraph{Примечание} Утверждается, что гауссовские процессы работают хорошо, если $k$ не очень большое.
Чем больше у нас $k$, тем сложнее посчитать $p(f(\theta) \mid \theta_{1}, \ldots, \theta_{k}, f(\theta_{1}), \ldots, f(\theta_{k}))$, поскольку нам нужно обращать большую матрицу ковариаций, и тем вычислительно сложнее становится оптимизация.


\paragraph{Ещё одно примечание} Если мы хотим попробовать больше разных гиперпараметров, то можно увеличить $\eta$.
По умолчанию $\eta$ берут как лучшее значение $f$.

\subsection{Tree-structured Parzen Estimator}

Допустим мы уже попробовали какое-то количество гиперпараметров $\theta_{1}, \ldots, \theta_{k}$ и посчитали $y_{1} = f(\theta_{1}), \ldots, y_{k} = f(\theta_{k})$.
Построим три распределения:
\[
	p(y < y_{\star}), \quad p(\theta \mid y < y_{\star}), \quad p(\theta \mid y \geqslant y_{\star}),
\]
где:
\begin{itemize}
	\item Первое распределение задаёт априорную вероятность того, что мы сможем найти такой набор гиперпараметров, который сильнее улучшит значение $f$ (например, 0.15);
	\item Второе распределение задаёт распределение на гиперпараметрах при условии, что мы смогли улучшить ошибку;
	\item Третье распределение задаёт распределение на гиперпараметрах при условии, что мы не смогли улучшить ошибку.
\end{itemize}

Для приближения второго и третьего распределений мы просто берем набор гиперпараметров $\theta$, который удовлетворяет условию, и строим непараметрическую оценку плотности.

По формуле Байеса можно найти $p(y \mid \theta)$, подставить в
\[
	\mathbb{E} \lambda (f(\theta)) \to \min\limits_{\theta}
\]
и найти нужный параметр $\theta_{k + 1}$.

\paragraph{Примечание} Метод называется \enquote{Tree-structured}, потому что обычно используется для случаев, когда одни гиперпараметры зависят от других.
Например, при подборе количества нейронов в слое нейросети, искомое число может зависеть от числа нейронов в предыдущем слое.


\end{document}