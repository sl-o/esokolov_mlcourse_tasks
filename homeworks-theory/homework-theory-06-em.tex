\documentclass[12pt,fleqn]{article}
\usepackage{../lecture-notes/vkCourseML}

\theorembodyfont{\rmfamily}
\newtheorem{esProblem}{Задача}

\title{Машинное обучение, ФКН ВШЭ\\Теоретическое домашнее задание №6}
\author{}
\date{}

\begin{document}
\maketitle

\begin{esProblem}
    Для двух одномерных нормальных распределений $\mathcal{N}(x\cond \mu_1, \sigma_1), \; \mathcal{N}(x\cond \mu_2, \sigma_2)$ найдите дивергенцию Кульбака-Лейблера:
    \begin{equation*}
        \text{KL}(\mathcal{N}(x\cond \mu_1, \sigma_1)\Vert \; \mathcal{N}(x\cond \mu_2, \sigma_2))
    \end{equation*}
\end{esProblem}

\begin{esProblem}
    Рассмотрим метод восстановления плотности распределения с помощью гистограмм.
    Разобьем все пространство на непересекающиеся области $\delta_i$.
    Каждому $\delta_i$ ставится в соответствие вероятность $h_i$.
    По заданной выборке $\{x_i\}_{i=1}^\ell$, найдите оптимальные значения $h_i$ с помощью метода максимального правдоподобия.
\end{esProblem}

\begin{esProblem}
    Рассмотрим общую схему EM-алгоритма,
    выводимую через разложение
    \[
        \log p(X \cond \Theta)
        =
        \LL(q, \Theta)
        +
        \KL{q}{p}.
    \]
    На E-шаге ищется распределение~$q$,
    доставляющее максимум нижней оценке~$\LL(q, \Theta^\text{old})$
    при фиксированном~$\Theta^\text{old}$.

    Модифицируем E-шаг: будем теперь искать максимум не среди всех
    возможных распределений, а лишь среди вырожденных,
    то есть присваивающих единичную вероятность одной точке
    и нулевую вероятность всем остальным.
    Как будут выглядеть E- и M-шаги в этом случае?
\end{esProblem}

\begin{esProblem}
    Наблюдается выборка бинарных значений $y = (y_1, \ldots, y_n), \; y_i \in \{0,1\}$.
    Все элементы выборки генерируются независимо, но известно, что в некоторый момент~$z$ меняется частота генерации единиц.
    Т.е., для всех~$i < z$ выполнено~$P(y_i=1) = \theta_1$, а для всех~$i \geq z$ выполнено~$P(y_i=1) = \theta_2$.
    Необходимо вывести формулы для ЕМ-алгоритма, где $z$~--- скрытая переменная, а $\theta_1, \theta_2$~--- параметры распределений.
\end{esProblem}

\begin{esProblem}
    Новогодние праздники подошли к концу. Все семинаристы курса по~МО-2 хорошо кушали и теперь хотят похудеть. Вес семинариста имеет распределение $x_i \sim \mathcal{N}(0, \sigma^2).$ Весы работают с погрешностью $\varepsilon_i \sim \mathcal{N}(0,1).$ После взвешивания каждый семинарист видит величину  $y_i = x_i + \varepsilon_i.$ 
    \begin{enumerate} 
        \item[а)] Найдите оценку максимального правдоподобия для $\sigma^2.$ Выразите её через $y_1, \ldots, y_{\ell}.$
        
        \item[б)] Семинаристы хотят оценить $\sigma^2$ с помощью EM-алгоритма. Выпишите $E$-шаг и $M-$шаг для нашей задачи. Найдите формулу пересчёта $\sigma^2_t$ в $\sigma^2_{t+1}.$ Найдите предел $\lim_{t \to \infty} \sigma^2_t.$
        
        \item[в)] Предложите семинаристам способ выяснить с помощью $EM-$ алгоритма их настоящий вес. 
    \end{enumerate}
\end{esProblem}

\begin{esProblem}
    Пусть мы пытаемся предсказать переменную-счётчик с аномальным значением в нуле. Например, это может быть количество рыб, пойманных на рыбалке. Чаще всего это ноль. Если это не ноль, то это счётчик, который распределён по Пуассону. Такую модель называют \textit{моделью с нулевым вздутием (zero inflated model)}:
    \begin{equation*}
        \begin{aligned}
            & P(y_i = 0) = p(x_i) + (1 - p(x_i))\cdot e^{-\lambda(x_i)}\\
            & P(y_i = k) = (1 - p(x_i))\cdot \frac{\lambda(x_i)^k \cdot e^{-\lambda(x_i)}}{k!}.
        \end{aligned}
    \end{equation*}
    
    Под $\lambda(x_i)$ и $p(x_i)$ имеются в виду какие-то зависимости от факторов. Например, может быть $\lambda(x_i) = \langle w, x_i \rangle$, а $p(x_i)$ --- логистическая регрессия. Если $p(x_i) = 0,$ получается пуассоновская регрессия. 
    
    Руководствуясь принципом максимизации правдоподобия, получите для такой модели функцию потерь для оптимизации.
\end{esProblem}





\end{document} 
